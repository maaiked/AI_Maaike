{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "NLP_Training (1).ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN78wBwRnP2h",
        "outputId": "fec857e0-3203-4259-c2cd-4c6e12153d34"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import csv\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "## NLTK\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "\n",
        "\n",
        "## Sklearn\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "## Spacy\n",
        "!pip install spacy && python -m spacy download en\n",
        "import spacy\n",
        "\n",
        "## GenSim\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models.tfidfmodel import TfidfModel"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (51.1.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.1.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msLjeg4IbHSj"
      },
      "source": [
        "# Nieuwe sectie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asv5Wg18LF6a"
      },
      "source": [
        "str_val = \"Dit is een testzin om te oefenen op het NLTK pakket.\\\r\n",
        "Ik heb geen idee wat het resultaat hiervan zal zijn.\\\r\n",
        "Ik hoop genoeg om geoefend mijn examen af te leggen :-) .\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5vw6FyRLTAB",
        "outputId": "51b75678-80f5-4dea-8077-6bd20e6ee272"
      },
      "source": [
        "# Splits tekst in zinnen\r\n",
        "zinnen_val = re.split('\\.', str_val)\r\n",
        "print (zinnen_val)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dit is een testzin om te oefenen op het NLTK pakket', 'Ik heb geen idee wat het resultaat hiervan zal zijn', 'Ik hoop genoeg om geoefend mijn examen af te leggen :-) ', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "derUjWvVMFQ-",
        "outputId": "e62ab954-1b78-4087-bdc6-84a71ea77568"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\r\n",
        "# Splits tekst in woorden met tokenize\r\n",
        "words_val = word_tokenize(str_val)\r\n",
        "print (words_val)\r\n",
        "# Splits tekst in woorden 2\r\n",
        "val3 = re.findall('(\\d+|\\w+)', str_val)\r\n",
        "print(val3)\r\n",
        "\r\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\r\n",
        "tokens = tokenizer.tokenize(str_val)\r\n",
        "print(tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dit', 'is', 'een', 'testzin', 'om', 'te', 'oefenen', 'op', 'het', 'NLTK', 'pakket.Ik', 'heb', 'geen', 'idee', 'wat', 'het', 'resultaat', 'hiervan', 'zal', 'zijn.Ik', 'hoop', 'genoeg', 'om', 'geoefend', 'mijn', 'examen', 'af', 'te', 'leggen', ':', '-', ')', '.']\n",
            "['Dit', 'is', 'een', 'testzin', 'om', 'te', 'oefenen', 'op', 'het', 'NLTK', 'pakket', 'Ik', 'heb', 'geen', 'idee', 'wat', 'het', 'resultaat', 'hiervan', 'zal', 'zijn', 'Ik', 'hoop', 'genoeg', 'om', 'geoefend', 'mijn', 'examen', 'af', 'te', 'leggen']\n",
            "['Dit', 'is', 'een', 'testzin', 'om', 'te', 'oefenen', 'op', 'het', 'NLTK', 'pakket', 'Ik', 'heb', 'geen', 'idee', 'wat', 'het', 'resultaat', 'hiervan', 'zal', 'zijn', 'Ik', 'hoop', 'genoeg', 'om', 'geoefend', 'mijn', 'examen', 'af', 'te', 'leggen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_VuBtlmNQgV",
        "outputId": "49673fec-82f4-4033-d180-e4c1a9663d00"
      },
      "source": [
        "# Splits enkel eerste zin in woorden\r\n",
        "words_val = re.findall('(\\d+|\\w+)', zinnen_val[0])\r\n",
        "print(words_val)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Dit', 'is', 'een', 'testzin', 'om', 'te', 'oefenen', 'op', 'het', 'NLTK', 'pakket']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oErgwiKGOCLd",
        "outputId": "ba885ab6-44f1-42fe-a2c6-b1a25762c7e6"
      },
      "source": [
        "# Alle woorden naar kleine letters zetten om correct te kunnen tellen\r\n",
        "nieuwetekst = str_val.lower()\r\n",
        "print(nieuwetekst)\r\n",
        "tokens = re.findall('(\\d+|\\w+)', nieuwetekst)\r\n",
        "print (tokens)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dit is een testzin om te oefenen op het nltk pakket.ik heb geen idee wat het resultaat hiervan zal zijn.ik hoop genoeg om geoefend mijn examen af te leggen :-) .\n",
            "['dit', 'is', 'een', 'testzin', 'om', 'te', 'oefenen', 'op', 'het', 'nltk', 'pakket', 'ik', 'heb', 'geen', 'idee', 'wat', 'het', 'resultaat', 'hiervan', 'zal', 'zijn', 'ik', 'hoop', 'genoeg', 'om', 'geoefend', 'mijn', 'examen', 'af', 'te', 'leggen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDDUNVDdPgt7",
        "outputId": "e2ef8a3c-c406-43df-abc0-4a240f63dd32"
      },
      "source": [
        "# Tel aantal woorden\r\n",
        "counter = Counter(tokens)\r\n",
        "print(counter)\r\n",
        "counter2 = Counter(sorted(tokens))\r\n",
        "print(counter2)\r\n",
        "\r\n",
        "counter.most_common(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'om': 2, 'te': 2, 'het': 2, 'ik': 2, 'dit': 1, 'is': 1, 'een': 1, 'testzin': 1, 'oefenen': 1, 'op': 1, 'nltk': 1, 'pakket': 1, 'heb': 1, 'geen': 1, 'idee': 1, 'wat': 1, 'resultaat': 1, 'hiervan': 1, 'zal': 1, 'zijn': 1, 'hoop': 1, 'genoeg': 1, 'geoefend': 1, 'mijn': 1, 'examen': 1, 'af': 1, 'leggen': 1})\n",
            "Counter({'het': 2, 'ik': 2, 'om': 2, 'te': 2, 'af': 1, 'dit': 1, 'een': 1, 'examen': 1, 'geen': 1, 'genoeg': 1, 'geoefend': 1, 'heb': 1, 'hiervan': 1, 'hoop': 1, 'idee': 1, 'is': 1, 'leggen': 1, 'mijn': 1, 'nltk': 1, 'oefenen': 1, 'op': 1, 'pakket': 1, 'resultaat': 1, 'testzin': 1, 'wat': 1, 'zal': 1, 'zijn': 1})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('om', 2), ('te', 2), ('het', 2), ('ik', 2), ('dit', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDwealninP2h"
      },
      "source": [
        "## Regular Expressions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rQx6R5pYnP2h"
      },
      "source": [
        "str_val = 'Let us try some NLP '\\\n",
        "            'all together during this last session. '\\\n",
        "            'The exercises will not be so difficult '\\\n",
        "            'and the end is near for us which makes it fun!' \\\n",
        "            'The question will we be successful? '\\\n",
        "            'And will there be 1 student who dares to ask questions?' \\\n",
        "            'This student would be very brave.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xANzfkH3nP2h",
        "outputId": "18898434-00bd-4644-8841-7167bf040887"
      },
      "source": [
        "val1 = re.split('\\.', str_val)\n",
        "print(val1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Let us try some NLP all together during this last session', ' The exercises will not be so difficult and the end is near for us which makes it fun!The question will we be successful? And will there be 1 student who dares to ask questions?This student would be very brave', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42FOUYY6nP2h",
        "outputId": "36004a61-82e4-4980-9560-f331a7d92fb8"
      },
      "source": [
        "val2 = re.findall('[A-Z]', str_val)\n",
        "print(val2)\n",
        "val3 = re.findall('(\\d+|\\w+)', str_val)\n",
        "print(val3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['L', 'N', 'L', 'P', 'T', 'T', 'A', 'T']\n",
            "['Let', 'us', 'try', 'some', 'NLP', 'all', 'together', 'during', 'this', 'last', 'session', 'The', 'exercises', 'will', 'not', 'be', 'so', 'difficult', 'and', 'the', 'end', 'is', 'near', 'for', 'us', 'which', 'makes', 'it', 'fun', 'The', 'question', 'will', 'we', 'be', 'successful', 'And', 'will', 'there', 'be', '1', 'student', 'who', 'dares', 'to', 'ask', 'questions', 'This', 'student', 'would', 'be', 'very', 'brave']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm9mjWhVnP2h",
        "outputId": "9f0dc017-35a2-43ce-f823-862e055fbb30"
      },
      "source": [
        "val4 = re.search('\\d', str_val)\n",
        "print(val4)\n",
        "print(val4.start(), val4.end())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(198, 199), match='1'>\n",
            "198 199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA8HYJbpnP2h",
        "outputId": "ea2afdde-2ca9-42fb-f777-9319c5d63a93"
      },
      "source": [
        "val5 = re.match('[A-Z]', str_val)\n",
        "print(val5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(0, 1), match='L'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5HH7yHwnP2i"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "NLTK Tokenization\n",
        "https://www.nltk.org/api/nltk.tokenize.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrgSQb8ZnP2i",
        "outputId": "b4b68a0a-8b3d-409f-caae-9161bd60f151"
      },
      "source": [
        "sent_tokenize(str_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let us try some NLP all together during this last session.',\n",
              " 'The exercises will not be so difficult and the end is near for us which makes it fun!The question will we be successful?',\n",
              " 'And will there be 1 student who dares to ask questions?This student would be very brave.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srAYVUDYnP2i",
        "outputId": "7e1e69ac-f910-4f82-fdb8-98cd8bddf832"
      },
      "source": [
        "regexp_tokenize(str_val, '[a-z]*')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " 'et',\n",
              " '',\n",
              " 'us',\n",
              " '',\n",
              " 'try',\n",
              " '',\n",
              " 'some',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'all',\n",
              " '',\n",
              " 'together',\n",
              " '',\n",
              " 'during',\n",
              " '',\n",
              " 'this',\n",
              " '',\n",
              " 'last',\n",
              " '',\n",
              " 'session',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'he',\n",
              " '',\n",
              " 'exercises',\n",
              " '',\n",
              " 'will',\n",
              " '',\n",
              " 'not',\n",
              " '',\n",
              " 'be',\n",
              " '',\n",
              " 'so',\n",
              " '',\n",
              " 'difficult',\n",
              " '',\n",
              " 'and',\n",
              " '',\n",
              " 'the',\n",
              " '',\n",
              " 'end',\n",
              " '',\n",
              " 'is',\n",
              " '',\n",
              " 'near',\n",
              " '',\n",
              " 'for',\n",
              " '',\n",
              " 'us',\n",
              " '',\n",
              " 'which',\n",
              " '',\n",
              " 'makes',\n",
              " '',\n",
              " 'it',\n",
              " '',\n",
              " 'fun',\n",
              " '',\n",
              " '',\n",
              " 'he',\n",
              " '',\n",
              " 'question',\n",
              " '',\n",
              " 'will',\n",
              " '',\n",
              " 'we',\n",
              " '',\n",
              " 'be',\n",
              " '',\n",
              " 'successful',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'nd',\n",
              " '',\n",
              " 'will',\n",
              " '',\n",
              " 'there',\n",
              " '',\n",
              " 'be',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'student',\n",
              " '',\n",
              " 'who',\n",
              " '',\n",
              " 'dares',\n",
              " '',\n",
              " 'to',\n",
              " '',\n",
              " 'ask',\n",
              " '',\n",
              " 'questions',\n",
              " '',\n",
              " '',\n",
              " 'his',\n",
              " '',\n",
              " 'student',\n",
              " '',\n",
              " 'would',\n",
              " '',\n",
              " 'be',\n",
              " '',\n",
              " 'very',\n",
              " '',\n",
              " 'brave',\n",
              " '',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsKFzX8snP2i",
        "outputId": "3beebd7a-a8e8-4d42-c87b-dc8a1351d2e9"
      },
      "source": [
        "word_tokenize(str_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let',\n",
              " 'us',\n",
              " 'try',\n",
              " 'some',\n",
              " 'NLP',\n",
              " 'all',\n",
              " 'together',\n",
              " 'during',\n",
              " 'this',\n",
              " 'last',\n",
              " 'session',\n",
              " '.',\n",
              " 'The',\n",
              " 'exercises',\n",
              " 'will',\n",
              " 'not',\n",
              " 'be',\n",
              " 'so',\n",
              " 'difficult',\n",
              " 'and',\n",
              " 'the',\n",
              " 'end',\n",
              " 'is',\n",
              " 'near',\n",
              " 'for',\n",
              " 'us',\n",
              " 'which',\n",
              " 'makes',\n",
              " 'it',\n",
              " 'fun',\n",
              " '!',\n",
              " 'The',\n",
              " 'question',\n",
              " 'will',\n",
              " 'we',\n",
              " 'be',\n",
              " 'successful',\n",
              " '?',\n",
              " 'And',\n",
              " 'will',\n",
              " 'there',\n",
              " 'be',\n",
              " '1',\n",
              " 'student',\n",
              " 'who',\n",
              " 'dares',\n",
              " 'to',\n",
              " 'ask',\n",
              " 'questions',\n",
              " '?',\n",
              " 'This',\n",
              " 'student',\n",
              " 'would',\n",
              " 'be',\n",
              " 'very',\n",
              " 'brave',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYkKdm7bnP2i",
        "outputId": "865788b6-20cd-4e2c-e3a4-4700ac2bbf5f"
      },
      "source": [
        "tknzr = TweetTokenizer()\n",
        "tweet = \"@cmiddag: This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
        "\n",
        "tknzr.tokenize(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@cmiddag',\n",
              " ':',\n",
              " 'This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'cooool',\n",
              " '#dummysmiley',\n",
              " ':',\n",
              " ':-)',\n",
              " ':-P',\n",
              " '<3',\n",
              " 'and',\n",
              " 'some',\n",
              " 'arrows',\n",
              " '<',\n",
              " '>',\n",
              " '->',\n",
              " '<--']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYurFVrqnP2i",
        "outputId": "0e6ecfb9-39a2-4eff-9da6-697e1ef82b20"
      },
      "source": [
        "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tknzr.tokenize(tweet)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[':',\n",
              " 'This',\n",
              " 'is',\n",
              " 'a',\n",
              " 'coool',\n",
              " '#dummysmiley',\n",
              " ':',\n",
              " ':-)',\n",
              " ':-P',\n",
              " '<3',\n",
              " 'and',\n",
              " 'some',\n",
              " 'arrows',\n",
              " '<',\n",
              " '>',\n",
              " '->',\n",
              " '<--']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIBu-Vu5nP2i",
        "outputId": "f6c4a940-ba2c-4a5d-b363-e88b50a41080"
      },
      "source": [
        "regexp_tokenize(tweet, \"([@#]\\w+)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@cmiddag', '#dummysmiley']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlTWomMEnP2i"
      },
      "source": [
        "## Charting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "nk9JTY34nP2i",
        "outputId": "d28249eb-69ff-4e11-ccb2-09d3e04d70ae"
      },
      "source": [
        "words = word_tokenize(str_val)\n",
        "\n",
        "print(words)\n",
        "\n",
        "word_lengths = [len(w) for w in words]\n",
        "\n",
        "print(word_lengths)\n",
        "\n",
        "plt.hist(word_lengths)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Let', 'us', 'try', 'some', 'NLP', 'all', 'together', 'during', 'this', 'last', 'session', '.', 'The', 'exercises', 'will', 'not', 'be', 'so', 'difficult', 'and', 'the', 'end', 'is', 'near', 'for', 'us', 'which', 'makes', 'it', 'fun', '!', 'The', 'question', 'will', 'we', 'be', 'successful', '?', 'And', 'will', 'there', 'be', '1', 'student', 'who', 'dares', 'to', 'ask', 'questions', '?', 'This', 'student', 'would', 'be', 'very', 'brave', '.']\n",
            "[3, 2, 3, 4, 3, 3, 8, 6, 4, 4, 7, 1, 3, 9, 4, 3, 2, 2, 9, 3, 3, 3, 2, 4, 3, 2, 5, 5, 2, 3, 1, 3, 8, 4, 2, 2, 10, 1, 3, 4, 5, 2, 1, 7, 3, 5, 2, 3, 9, 1, 4, 7, 5, 2, 4, 5, 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b_5pJSn4nP2i"
      },
      "source": [
        "## Bag-of-words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiW08NounP2i",
        "outputId": "5e8c6fb3-1ed3-4310-ef59-163f39b7ef20"
      },
      "source": [
        "str_val2 = \"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"\n",
        "\n",
        "counter = Counter(word_tokenize(str_val2))\n",
        "print(counter)\n",
        "\n",
        "counter.most_common(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 3), ('cat', 3), ('the', 3), ('box', 3), ('.', 3)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KfrZPiounP2i"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAodL_mInP2i",
        "outputId": "5260f227-3d13-47f5-ebc5-d48bb2b2113a"
      },
      "source": [
        "tokens = [w for w in word_tokenize(str_val.lower())\n",
        "         if w.isalpha()]\n",
        "\n",
        "print(tokens)\n",
        "print(stopwords.words('english'))\n",
        "no_stops = [t for t in tokens\n",
        "           if t not in stopwords.words('english')]\n",
        "\n",
        "print(no_stops)\n",
        "\n",
        "Counter(no_stops).most_common(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['let', 'us', 'try', 'some', 'nlp', 'all', 'together', 'during', 'this', 'last', 'session', 'the', 'exercises', 'will', 'not', 'be', 'so', 'difficult', 'and', 'the', 'end', 'is', 'near', 'for', 'us', 'which', 'makes', 'it', 'fun', 'the', 'question', 'will', 'we', 'be', 'successful', 'and', 'will', 'there', 'be', 'student', 'who', 'dares', 'to', 'ask', 'questions', 'this', 'student', 'would', 'be', 'very', 'brave']\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
            "['let', 'us', 'try', 'nlp', 'together', 'last', 'session', 'exercises', 'difficult', 'end', 'near', 'us', 'makes', 'fun', 'question', 'successful', 'student', 'dares', 'ask', 'questions', 'student', 'would', 'brave']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('us', 2), ('student', 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htYuk__AnP2i",
        "outputId": "ea1ca168-f96c-46eb-c3f1-9f2b558ca707"
      },
      "source": [
        "\n",
        "\n",
        "tokens = [w for w in word_tokenize(str_val.lower()) if w.isalpha()]\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['let', 'us', 'try', 'some', 'nlp', 'all', 'together', 'during', 'this', 'last', 'session', 'the', 'exercises', 'will', 'not', 'be', 'so', 'difficult', 'and', 'the', 'end', 'is', 'near', 'for', 'us', 'which', 'makes', 'it', 'fun', 'the', 'question', 'will', 'we', 'be', 'successful', 'and', 'will', 'there', 'be', 'student', 'who', 'dares', 'to', 'ask', 'questions', 'this', 'student', 'would', 'be', 'very', 'brave']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSoz9TmWnP2i",
        "outputId": "71e9b1f6-54fc-47fe-bd26-36117980c535"
      },
      "source": [
        "no_stops = [t for t in tokens if t not in stopwords.words('english')]\n",
        "print(no_stops)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['let', 'us', 'try', 'nlp', 'together', 'last', 'session', 'exercises', 'difficult', 'end', 'near', 'us', 'makes', 'fun', 'question', 'successful', 'student', 'dares', 'ask', 'questions', 'student', 'would', 'brave']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JByPMmgznP2i",
        "outputId": "219ae73b-ccda-477f-8fcd-bef63c5eeda3"
      },
      "source": [
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "print(lemmatized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['let', 'u', 'try', 'nlp', 'together', 'last', 'session', 'exercise', 'difficult', 'end', 'near', 'u', 'make', 'fun', 'question', 'successful', 'student', 'dare', 'ask', 'question', 'student', 'would', 'brave']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6YOTlCQnP2i",
        "outputId": "bdd92079-b4d2-4147-d4d7-77389c0b7a9e"
      },
      "source": [
        "Counter(lemmatized)\n",
        "Counter(lemmatized).most_common(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('u', 2), ('question', 2), ('student', 2), ('let', 1), ('try', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC0Swgw2nP2j"
      },
      "source": [
        "## GenSim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d9RdxmA1nP2j"
      },
      "source": [
        "my_documents = ['The movie was about a spaceship and aliens.',\n",
        "    'I really liked the movie!',\n",
        "    'Awesome action scenes, but boring characters.',\n",
        "    'The movie was awful! I hate alien films.',\n",
        "    'Space is cool! I liked the movie.',\n",
        "    'More space films, please!']\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower())\n",
        "                 for doc in my_documents]\n",
        "\n",
        "dictionary = Dictionary(tokenized_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryQ_VtmznP2j",
        "outputId": "175cd8c4-41e4-48a4-e656-337cafcbee56"
      },
      "source": [
        "dictionary.token2id"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 9,\n",
              " ',': 13,\n",
              " '.': 0,\n",
              " 'a': 1,\n",
              " 'about': 2,\n",
              " 'action': 14,\n",
              " 'alien': 20,\n",
              " 'aliens': 3,\n",
              " 'and': 4,\n",
              " 'awesome': 15,\n",
              " 'awful': 21,\n",
              " 'boring': 16,\n",
              " 'but': 17,\n",
              " 'characters': 18,\n",
              " 'cool': 24,\n",
              " 'films': 22,\n",
              " 'hate': 23,\n",
              " 'i': 10,\n",
              " 'is': 25,\n",
              " 'liked': 11,\n",
              " 'more': 27,\n",
              " 'movie': 5,\n",
              " 'please': 28,\n",
              " 'really': 12,\n",
              " 'scenes': 19,\n",
              " 'space': 26,\n",
              " 'spaceship': 6,\n",
              " 'the': 7,\n",
              " 'was': 8}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KT6NTXMNnP2j"
      },
      "source": [
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGbikSPnnP2j",
        "outputId": "2e129eb0-ecfe-49da-ed8b-4d4ee5e204ab"
      },
      "source": [
        "corpus"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
              " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
              " [(0, 1),\n",
              "  (5, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (20, 1),\n",
              "  (21, 1),\n",
              "  (22, 1),\n",
              "  (23, 1)],\n",
              " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
              " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIiBEPIVnP2j"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "n2vXqSManP2j"
      },
      "source": [
        "tfidf = TfidfModel(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlIHBMAunP2j",
        "outputId": "566099a4-171a-41eb-db70-e050a957c342"
      },
      "source": [
        "tfidf[corpus[3]]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 0.11167183378630395),\n",
              " (5, 0.11167183378630395),\n",
              " (7, 0.11167183378630395),\n",
              " (8, 0.30257609456991347),\n",
              " (9, 0.11167183378630395),\n",
              " (10, 0.19090426078360948),\n",
              " (20, 0.4934803553535229),\n",
              " (21, 0.4934803553535229),\n",
              " (22, 0.30257609456991347),\n",
              " (23, 0.4934803553535229)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tchusU9dnP2j"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "d4rn5x1bnP2j"
      },
      "source": [
        "sentence = '''In New York, I like to ride the Metro to visit MOMA \n",
        "                      and some restaurants rated well by Ruth Reichl.'''\n",
        "\n",
        "tokenized_sent = nltk.word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCjL5M8ynP2j",
        "outputId": "330c7b45-b361-4344-a538-8ab0cefa313f"
      },
      "source": [
        "tagged_sent = nltk.pos_tag(tokenized_sent)\n",
        "print(tagged_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('In', 'IN'), ('New', 'NNP'), ('York', 'NNP'), (',', ','), ('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('ride', 'VB'), ('the', 'DT'), ('Metro', 'NNP'), ('to', 'TO'), ('visit', 'VB'), ('MOMA', 'NNP'), ('and', 'CC'), ('some', 'DT'), ('restaurants', 'NNS'), ('rated', 'VBN'), ('well', 'RB'), ('by', 'IN'), ('Ruth', 'NNP'), ('Reichl', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m49duRinP2j"
      },
      "source": [
        " - CC coordinating conjunction\n",
        " - CD cardinal digit\n",
        " - DT determiner\n",
        " - EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
        " - FW foreign word\n",
        " - IN preposition/subordinating conjunction\n",
        " - JJ adjective 'big'\n",
        " - JJR adjective, comparative 'bigger'\n",
        " - JJS adjective, superlative 'biggest'\n",
        " - LS list marker 1)\n",
        " - MD modal could, will\n",
        " - NN noun, singular 'desk'\n",
        " - NNS noun plural 'desks'\n",
        " - NNP proper noun, singular 'Harrison'\n",
        " - NNPS proper noun, plural 'Americans'\n",
        " - PDT predeterminer 'all the kids'\n",
        " - POS possessive ending parent's\n",
        " - PRP personal pronoun I, he, she\n",
        " - PRP\\$ possessive pronoun my, his, hers\n",
        " - RB adverb very, silently,\n",
        " - RBR adverb, comparative better\n",
        " - RBS adverb, superlative best\n",
        " - RP particle give up\n",
        " - TO to go 'to' the store.\n",
        " - UH interjection errrrrrrrm\n",
        " - VB verb, base form take\n",
        " - VBD verb, past tense took\n",
        " - VBG verb, gerund/present participle taking\n",
        " - VBN verb, past participle taken\n",
        " - VBP verb, sing. present, non-3d take\n",
        " - VBZ verb, 3rd person sing. present takes\n",
        " - WDT wh-determiner which\n",
        " - WP wh-pronoun who, what\n",
        " - WP\\$ possessive wh-pronoun whose\n",
        " - WRB wh-abverb where, when"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_d2JqLsSnP2j",
        "outputId": "79b721d4-f45f-4f2b-b2e6-112f0baa5a69"
      },
      "source": [
        "print(nltk.ne_chunk(tagged_sent))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  In/IN\n",
            "  (GPE New/NNP York/NNP)\n",
            "  ,/,\n",
            "  I/PRP\n",
            "  like/VBP\n",
            "  to/TO\n",
            "  ride/VB\n",
            "  the/DT\n",
            "  (ORGANIZATION Metro/NNP)\n",
            "  to/TO\n",
            "  visit/VB\n",
            "  (ORGANIZATION MOMA/NNP)\n",
            "  and/CC\n",
            "  some/DT\n",
            "  restaurants/NNS\n",
            "  rated/VBN\n",
            "  well/RB\n",
            "  by/IN\n",
            "  (PERSON Ruth/NNP Reichl/NNP)\n",
            "  ./.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwUvz4dZnP2j"
      },
      "source": [
        "## Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MELE4hTInP2j",
        "outputId": "2431ab4a-580d-41aa-d4db-6a980d18bdae"
      },
      "source": [
        "nlp = spacy.load('en')\n",
        "\n",
        "nlp.entity\n",
        "\n",
        "doc = nlp(\"\"\"Berlin is the capital of Germany; \n",
        "                  and the residence of Chancellor Angela Merkel.\"\"\")\n",
        "print(doc)\n",
        "\n",
        "print(doc.ents[0], doc.ents[0].label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Berlin is the capital of Germany; \n",
            "                  and the residence of Chancellor Angela Merkel.\n",
            "Berlin GPE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hIe9NEMnP2j"
      },
      "source": [
        "## IMDB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "NfUILFS5nP2j"
      },
      "source": [
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 300)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "Gwl2Q4XFnP2k",
        "outputId": "f3e940a8-a2e6-49f1-d33d-89c022f119d1"
      },
      "source": [
        "meta = pd.read_csv(\"movie.metadata.tsv\", sep = '\\t', header = None)\n",
        "# rename columns\n",
        "meta.columns = [\"movie_id\",1,\"movie_name\",3,4,5,6,7,\"genre\"]\n",
        "\n",
        "meta.head()\n",
        "\n",
        "plots = []\n",
        "\n",
        "with open(\"plot_summaries.txt\", 'r') as f:\n",
        "       reader = csv.reader(f, dialect='excel-tab') \n",
        "       for row in tqdm(reader):\n",
        "            plots.append(row)\n",
        "            \n",
        "\n",
        "movie_id = []\n",
        "plot = []\n",
        "\n",
        "# extract movie Ids and plot summaries\n",
        "for i in tqdm(plots):\n",
        "  movie_id.append(i[0])\n",
        "  plot.append(i[1])\n",
        "\n",
        "# create dataframe\n",
        "movies = pd.DataFrame({'movie_id': movie_id, 'plot': plot})"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-ea4d251a4214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"movie.metadata.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# rename columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"movie_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"movie_name\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"genre\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'movie.metadata.tsv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VQbLsszVnP2k"
      },
      "source": [
        "movie_id = []\n",
        "plot = []\n",
        "\n",
        "# extract movie Ids and plot summaries\n",
        "for i in tqdm(plots):\n",
        "  movie_id.append(i[0])\n",
        "  plot.append(i[1])\n",
        "\n",
        "# create dataframe\n",
        "movies = pd.DataFrame({'movie_id': movie_id, 'plot': plot})\n",
        "\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Fb3pyTo7nP2k"
      },
      "source": [
        "# change datatype of 'movie_id'\n",
        "meta['movie_id'] = meta['movie_id'].astype(str)\n",
        "\n",
        "# merge meta with movies\n",
        "movies = pd.merge(movies, meta[['movie_id', 'movie_name', 'genre']], on = 'movie_id')\n",
        "\n",
        "movies.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "e1rxqvRHnP2k"
      },
      "source": [
        "movies['genre'][0]\n",
        "\n",
        "type(json.loads(movies['genre'][0]))\n",
        "\n",
        "json.loads(movies['genre'][0]).values()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ya4omDCznP2k"
      },
      "source": [
        "# an empty list\n",
        "genres = [] \n",
        "\n",
        "# extract genres\n",
        "for i in movies['genre']: \n",
        "  genres.append(list(json.loads(i).values())) \n",
        "\n",
        "# add to 'movies' dataframe  \n",
        "movies['genre_new'] = genres\n",
        "\n",
        "print(genres)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "BOd7yS5TnP2k"
      },
      "source": [
        "# remove samples with 0 genre tags\n",
        "movies_new = movies[~(movies['genre_new'].str.len() == 0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W00P2LkKnP2k"
      },
      "source": [
        "movies_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bo6TKSkWnP2k"
      },
      "source": [
        "# get all genre tags in a list\n",
        "all_genres = sum(genres,[])\n",
        "len(set(all_genres))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "mrSWJpfRnP2k"
      },
      "source": [
        "all_genres = nltk.FreqDist(all_genres) \n",
        "\n",
        "# create dataframe\n",
        "all_genres_df = pd.DataFrame({'Genre': list(all_genres.keys()), \n",
        "                              'Count': list(all_genres.values())})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bkEUalgBnP2k"
      },
      "source": [
        "g = all_genres_df.nlargest(columns=\"Count\", n = 50) \n",
        "plt.figure(figsize=(12,15)) \n",
        "ax = sns.barplot(data=g, x= \"Count\", y = \"Genre\") \n",
        "ax.set(ylabel = 'Count') \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Yd-F4qHMnP2k"
      },
      "source": [
        "# function for text cleaning \n",
        "def clean_text(text):\n",
        "    # remove backslash-apostrophe \n",
        "    text = re.sub(\"\\'\", \"\", text) \n",
        "    # remove everything except alphabets \n",
        "    text = re.sub(\"[^a-zA-Z]\",\" \",text) \n",
        "    # remove whitespaces \n",
        "    text = ' '.join(text.split()) \n",
        "    # convert text to lowercase \n",
        "    text = text.lower() \n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uo8Wz8y4nP2k"
      },
      "source": [
        "movies_new['clean_plot'] = movies_new['plot'].apply(lambda x: clean_text(x))\n",
        "print(movies_new['clean_plot'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DAs86voPnP2k"
      },
      "source": [
        "def freq_words(x, terms = 30): \n",
        "  all_words = ' '.join([text for text in x]) \n",
        "  all_words = all_words.split() \n",
        "  fdist = nltk.FreqDist(all_words) \n",
        "  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())}) \n",
        "  \n",
        "  # selecting top 20 most frequent words \n",
        "  d = words_df.nlargest(columns=\"count\", n = terms) \n",
        "  \n",
        "  # visualize words and frequencies\n",
        "  plt.figure(figsize=(12,15)) \n",
        "  ax = sns.barplot(data=d, x= \"count\", y = \"word\") \n",
        "  ax.set(ylabel = 'Word') \n",
        "  plt.show()\n",
        "  \n",
        "# print 100 most frequent words \n",
        "freq_words(movies_new['clean_plot'], 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6L22qtyknP2k"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# function to remove stopwords\n",
        "def remove_stopwords(text):\n",
        "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
        "    return ' '.join(no_stopword_text)\n",
        "\n",
        "movies_new['clean_plot'] = movies_new['clean_plot'].apply(lambda x: remove_stopwords(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9ySGg1MZnP2k"
      },
      "source": [
        "freq_words(movies_new['clean_plot'], 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ka2_-vy-nP2k"
      },
      "source": [
        "multilabel_binarizer = MultiLabelBinarizer()\n",
        "multilabel_binarizer.fit(movies_new['genre_new'])\n",
        "\n",
        "# transform target variable\n",
        "y = multilabel_binarizer.transform(movies_new['genre_new'])\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "zCaylK_JnP2k"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aauwd0y0nP2k"
      },
      "source": [
        "# split dataset into training and validation set\n",
        "xtrain, xval, ytrain, yval = train_test_split(movies_new['clean_plot'], y, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qdX5dQ1WnP2k"
      },
      "source": [
        "# create TF-IDF features\n",
        "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\n",
        "xval_tfidf = tfidf_vectorizer.transform(xval)\n",
        "\n",
        "print(xtrain_tfidf)\n",
        "print(ytrain)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xmlK40oMnP2k"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "clf = OneVsRestClassifier(lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bLzfo5S1nP2k"
      },
      "source": [
        "# fit model on train data\n",
        "clf.fit(xtrain_tfidf, ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9BEl38NOnP2k"
      },
      "source": [
        "# make predictions for validation set\n",
        "y_pred = clf.predict(xval_tfidf)\n",
        "\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "i4UAnCa2nP2k"
      },
      "source": [
        "multilabel_binarizer.inverse_transform(y_pred)[3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uTJkYw3SnP2k"
      },
      "source": [
        "# evaluate performance\n",
        "f1_score(yval, y_pred, average=\"micro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aPs1DnmdnP2l"
      },
      "source": [
        "# predict probabilities\n",
        "y_pred_prob = clf.predict_proba(xval_tfidf)\n",
        "t = 0.3 # threshold value\n",
        "y_pred_new = (y_pred_prob >= t).astype(int)\n",
        "# evaluate performance\n",
        "f1_score(yval, y_pred_new, average=\"micro\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QyFEcQLjnP2l"
      },
      "source": [
        "\n",
        "def infer_tags(q):\n",
        "    q = clean_text(q)\n",
        "    q = remove_stopwords(q)\n",
        "    q_vec = tfidf_vectorizer.transform([q])\n",
        "    q_pred = clf.predict(q_vec)\n",
        "    return multilabel_binarizer.inverse_transform(q_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ucKUNlQMnP2l"
      },
      "source": [
        "for i in range(5): \n",
        "  k = xval.sample(1).index[0] \n",
        "  print(\"Movie: \", movies_new['movie_name'][k], \"\\nPredicted genre: \", infer_tags(xval[k])), print(\"Actual genre: \",movies_new['genre_new'][k], \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}