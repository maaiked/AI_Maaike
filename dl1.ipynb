{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "dl1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLi4TNbyxucL"
      },
      "source": [
        "# Deep Learning 1\n",
        "\n",
        "In this practical session, we will take a look at the [Keras](https://keras.io/) deep learning framework for Python. From the documentation:\n",
        "\n",
        ">Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
        "\n",
        "Designing and testing neural networks can be a tedious process. Keras provides an intuitive API for training, testing and deploying neural networks without having to worry too much about technical details. To begin, we import it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsuxLP5bxucM"
      },
      "source": [
        "import keras"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7a2TkRxucS"
      },
      "source": [
        "Keras can be used with different *backends*. The backend is another framework which takes care of the low-level details of the implementation, such as GPU optimization and distributed computing. At the time of this writing, Keras supports [TensorFlow](https://www.tensorflow.org/), [Theano](http://deeplearning.net/software/theano/) and [CNTK](https://www.microsoft.com/en-us/cognitive-toolkit/) backends. Upon importing the module, Keras will report which backend (if any) it is using.\n",
        "\n",
        "## Deep Neural Network basics\n",
        "\n",
        "A Deep Neural Network (DNN) is built up of several layers. Formally, such a network is given by\n",
        "\n",
        "$$\n",
        "    f(x) = g_L(W_Lg_{L-1}(\\dots g_1(W_1x + b_1) \\dots) + b_L)\n",
        "$$\n",
        "\n",
        "Here, $W_1, \\dots, W_L$ are matrices (called the *weights* of the network), $b_1, \\dots, b_L$ are vectors (called the *biases*) and $g_1, \\dots, g_L$ are the *activation functions*. We can picture this as follows:\n",
        "\n",
        "![A neural network](mlp.png)\n",
        "\n",
        "Each node in the network computes an inner product plus a bias term between its weight vector, which is a row of the weight matrix of that layer, and its input, which is the output of the previous layer. This inner product is then sent through a non-linear activation function. Examples of typical activation functions are\n",
        "\n",
        "1. the logistic sigmoid:\n",
        "$$\n",
        "    \\mathrm{sigmoid}(z) = \\frac{1}{1 + \\exp(-z)},\n",
        "$$\n",
        "\n",
        "2. the rectified linear unit (RELU):\n",
        "$$\n",
        "    \\mathrm{relu}(z) = \\max(0,z),\n",
        "$$\n",
        "\n",
        "3. the scaled exponential linear unit (SELU):\n",
        "$$\n",
        "    \\mathrm{selu}(z) = \\lambda\\left\\{\\begin{matrix}\n",
        "        z, & \\mbox{if $z > 0$}\\\\\n",
        "        \\alpha(\\exp(z)-1), & \\mbox{if $z \\leq 0$}\n",
        "    \\end{matrix}\\right..\n",
        "$$\n",
        "Here, $\\lambda$ and $\\alpha$ are parameters fixed before the network is trained.\n",
        "\n",
        "4. the softmax:\n",
        "$$\n",
        "    \\mathrm{softmax}(z) = \\frac{\\exp(z)}{\\sum_i\\exp(z_i)}.\n",
        "$$\n",
        "The softmax function is only used at the very end of a neural network classifier. It has the properties that for any $z \\in \\mathbb{R}^q$,\n",
        "$$\\begin{aligned}\n",
        "    \\mathrm{softmax}(z) &\\in [0,1]^q, & \\sum_{i=1}^q\\mathrm{softmax}(z)_i &= 1.\n",
        "\\end{aligned}$$\n",
        "In other words, the output of $\\mathrm{softmax}(z)$ can be interpreted as a vector of probabilities over $q$ possible outputs.\n",
        "\n",
        "We can construct a simple neural network in Keras as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUM5RZshxucS"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(units=64, activation='relu', input_dim=30),\n",
        "    Dense(units=10, activation='relu'),\n",
        "    Dense(units=2, activation='softmax')\n",
        "])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhAdkomcxucV"
      },
      "source": [
        "This model consists of three *dense* or *fully-connected* layers. These are the \"classical\" neural network layers which perform a linear transformation and then apply a specified activation function. In our case, the input layer has 30 dimensions, the hidden layer 64 nodes, the output layer 2.  This gives a total of \n",
        "\n",
        "$$\n",
        "    64 \\times 30 + 10 \\times 64 + 2 \\times 10 + 64 + 10 + 2 = 2656\n",
        "$$\n",
        "\n",
        "parameters. Of course, the weight matrices and bias vectors have not been set to any meaningful values yet. Finding values for these parameters such that the network maximizes a certain performance measure is the goal of a *learning algorithm*.\n",
        "\n",
        "## Supervised learning\n",
        "\n",
        "In the supervised learning setting, we are given a data set of observations, used to train the network.  Training the network aims to minimize the so called  *loss function* which measures how much the output $f(x)$ deviates from the desired output $y$. In a classification problem, usually the 0/1 loss is used:\n",
        "\n",
        "$$\n",
        "    \\ell(f(x),y) = \\left\\{\\begin{matrix}\n",
        "        1, & \\mbox{if $f(x) \\neq y$}\\\\\n",
        "        0, & \\mbox{if $f(x) = y$}\n",
        "    \\end{matrix}\\right..\n",
        "$$\n",
        "\n",
        "However, in some cases (e.g. regression problems) it makes sense to use other functions such as the squared error\n",
        "\n",
        "$$\n",
        "    \\ell(f(x),y) = \\|f(x)-y\\|_2^2.\n",
        "$$\n",
        "\n",
        "We now need to solve this optimization problem.  \n",
        "In practice, this means adjusting our weight matrices and bias vectors until we don't get anymore reductions in loss. There exist a variety of optimization algorithms to solve this problem, but we won't get into their details here. What is important to know is that all of these optimization algorithms work iteratively on *mini-batches* of the training data set, never on the entire training set at once unless it is really small (which it often isn't). The number of mini-batches is determined by the *batch size*, which is the size of a mini-batch. So the optimization algorithms proceed as follows:\n",
        "\n",
        "1. Split the training set into mini-batches with a number of samples equal to the batch size.\n",
        "2. For each mini-batch, compute updates to the weights and biases based solely on the given mini-batch data.\n",
        "3. Repeat step 2 until convergence.\n",
        "\n",
        "Each run of the loop in step 2 is called an *epoch*. You will always need to specify the number of epochs as well as the batch size whenever you train a neural network. Make sure that the batch size is always a divisor of the training set size, otherwise some algorithms may refuse to run. Typical batch sizes are small powers of two such as 64 or 128. In case the training set size happens to be a prime number (which can occur e.g. if you augment the training set with artificial samples), you may need to subsample.\n",
        "\n",
        "To train our model in Keras, we compile it with a loss function and an optimizer which will attempt to minimize the loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEhWKVVwxucW"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzmm2XWDxucY"
      },
      "source": [
        "Note that the `metrics` takes a list as an argument, because multiple metrics can be computed for the model. See [the documentation](https://keras.io/metrics/) for an exhaustive list of supported metrics. The model can now be fit to a data set. We will try the Wisconsin breast cancer data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkdJlkLBxucZ"
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "wisconsin = load_breast_cancer()\n",
        "x_data = wisconsin['data']\n",
        "y_data = wisconsin['target']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_6m0mU7xucb"
      },
      "source": [
        "The network expects our target labels to be 2-dimensional vectors of class probabilities, so we perform one-hot encoding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TuxHZz0xucc"
      },
      "source": [
        "y_data = np.array([[1., 0.] if y == 0 else [0., 1.] for y in y_data])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLnLnV6pxuce"
      },
      "source": [
        "We now shuffle and split the data set into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6T9ZQkrxucf"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "x_data, y_data = shuffle(x_data, y_data)\n",
        "\n",
        "p = .8\n",
        "idx = int(x_data.shape[0] * p)\n",
        "x_train, y_train = x_data[:idx], y_data[:idx]\n",
        "x_test, y_test = x_data[idx:], y_data[idx:]\n",
        "\n",
        "x_mean, x_std = x_train.mean(), x_train.std()\n",
        "x_train -= x_mean\n",
        "x_train /= x_std\n",
        "\n",
        "x_test -= x_mean\n",
        "x_test /= x_std"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiG1yfOpxuci"
      },
      "source": [
        "Note that we have z-normalized the samples to zero mean and unit variance. Now it's up to Keras to fit the model to this data set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3MmQFCJxuci",
        "outputId": "32225fb8-2035-44a1-95cd-8dbe708de8ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=100, batch_size=65)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.8114 - accuracy: 0.6220\n",
            "Epoch 2/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.6158 - accuracy: 0.7319\n",
            "Epoch 3/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.8659\n",
            "Epoch 4/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.5219 - accuracy: 0.8813\n",
            "Epoch 5/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.8879\n",
            "Epoch 6/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.4737 - accuracy: 0.9033\n",
            "Epoch 7/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.9033\n",
            "Epoch 8/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.4403 - accuracy: 0.8857\n",
            "Epoch 9/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8967\n",
            "Epoch 10/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.4122 - accuracy: 0.9033\n",
            "Epoch 11/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8989\n",
            "Epoch 12/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3899 - accuracy: 0.8923\n",
            "Epoch 13/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3804 - accuracy: 0.9099\n",
            "Epoch 14/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8967\n",
            "Epoch 15/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3613 - accuracy: 0.8989\n",
            "Epoch 16/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.9011\n",
            "Epoch 17/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3448 - accuracy: 0.9011\n",
            "Epoch 18/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.9143\n",
            "Epoch 19/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.9055\n",
            "Epoch 20/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3243 - accuracy: 0.9121\n",
            "Epoch 21/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.9077\n",
            "Epoch 22/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.9055\n",
            "Epoch 23/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3088 - accuracy: 0.9121\n",
            "Epoch 24/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.3036 - accuracy: 0.9099\n",
            "Epoch 25/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.9121\n",
            "Epoch 26/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2943 - accuracy: 0.9165\n",
            "Epoch 27/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2900 - accuracy: 0.9121\n",
            "Epoch 28/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2867 - accuracy: 0.9099\n",
            "Epoch 29/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.9143\n",
            "Epoch 30/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2794 - accuracy: 0.9187\n",
            "Epoch 31/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2765 - accuracy: 0.9077\n",
            "Epoch 32/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2726 - accuracy: 0.9077\n",
            "Epoch 33/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2717 - accuracy: 0.9077\n",
            "Epoch 34/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2668 - accuracy: 0.9121\n",
            "Epoch 35/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2643 - accuracy: 0.9121\n",
            "Epoch 36/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9143\n",
            "Epoch 37/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.9165\n",
            "Epoch 38/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.9077\n",
            "Epoch 39/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.9099\n",
            "Epoch 40/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.9143\n",
            "Epoch 41/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9077\n",
            "Epoch 42/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.9099\n",
            "Epoch 43/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9099\n",
            "Epoch 44/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.9077\n",
            "Epoch 45/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.9077\n",
            "Epoch 46/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.9121\n",
            "Epoch 47/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9099\n",
            "Epoch 48/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.9055\n",
            "Epoch 49/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2393 - accuracy: 0.9033\n",
            "Epoch 50/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2394 - accuracy: 0.9099\n",
            "Epoch 51/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2360 - accuracy: 0.9099\n",
            "Epoch 52/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2355 - accuracy: 0.9099\n",
            "Epoch 53/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2330 - accuracy: 0.9143\n",
            "Epoch 54/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9143\n",
            "Epoch 55/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2315 - accuracy: 0.9055\n",
            "Epoch 56/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9077\n",
            "Epoch 57/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.9055\n",
            "Epoch 58/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2283 - accuracy: 0.9099\n",
            "Epoch 59/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.9077\n",
            "Epoch 60/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2261 - accuracy: 0.9099\n",
            "Epoch 61/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2250 - accuracy: 0.9077\n",
            "Epoch 62/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9099\n",
            "Epoch 63/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2245 - accuracy: 0.9121\n",
            "Epoch 64/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2225 - accuracy: 0.9121\n",
            "Epoch 65/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2219 - accuracy: 0.9077\n",
            "Epoch 66/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9033\n",
            "Epoch 67/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2203 - accuracy: 0.9055\n",
            "Epoch 68/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.9077\n",
            "Epoch 69/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2194 - accuracy: 0.9121\n",
            "Epoch 70/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9099\n",
            "Epoch 71/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2177 - accuracy: 0.9033\n",
            "Epoch 72/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9099\n",
            "Epoch 73/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9077\n",
            "Epoch 74/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9099\n",
            "Epoch 75/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9077\n",
            "Epoch 76/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2157 - accuracy: 0.9077\n",
            "Epoch 77/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9143\n",
            "Epoch 78/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2147 - accuracy: 0.9143\n",
            "Epoch 79/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.9099\n",
            "Epoch 80/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9099\n",
            "Epoch 81/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2140 - accuracy: 0.9143\n",
            "Epoch 82/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2141 - accuracy: 0.9099\n",
            "Epoch 83/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2134 - accuracy: 0.9143\n",
            "Epoch 84/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9055\n",
            "Epoch 85/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2115 - accuracy: 0.9121\n",
            "Epoch 86/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9055\n",
            "Epoch 87/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2100 - accuracy: 0.9077\n",
            "Epoch 88/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9077\n",
            "Epoch 89/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9055\n",
            "Epoch 90/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.9143\n",
            "Epoch 91/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.9055\n",
            "Epoch 92/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9099\n",
            "Epoch 93/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2084 - accuracy: 0.9055\n",
            "Epoch 94/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9121\n",
            "Epoch 95/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2095 - accuracy: 0.9121\n",
            "Epoch 96/100\n",
            "7/7 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9077\n",
            "Epoch 97/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2082 - accuracy: 0.9033\n",
            "Epoch 98/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2061 - accuracy: 0.9099\n",
            "Epoch 99/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9099\n",
            "Epoch 100/100\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.2067 - accuracy: 0.9055\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f98f0f32e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmTs_wHxucl"
      },
      "source": [
        "Due to randomness in the initialization of the weights and biases as well as the optimization algorithm itself, repeated runs of the `fit` method will yield different results. You should be able to get at least 98% test accuracy after a few runs.\n",
        "\n",
        "Having trained the model, we can ask it for predictions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXqtERtVxucl"
      },
      "source": [
        "classes = np.argmax(model.predict(x_test, batch_size=65), axis=1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVzbQXSpxucp"
      },
      "source": [
        "Using this array, we can compute the accuracy ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuIEecvMxucp",
        "outputId": "b5789c9b-a9d9-4170-d414-b68fd1998d14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "accuracy = np.mean(np.equal(classes, np.argmax(y_test, axis=1)))\n",
        "print('Accuracy: {}'.format(accuracy))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.956140350877193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LMPTAlexucs"
      },
      "source": [
        "Or we could let Keras compute it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFQFT5hrxuct",
        "outputId": "0ad0d571-5887-4914-8f5e-72373b8395fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "evals = model.evaluate(x_test, y_test, batch_size=65)\n",
        "print('Loss: {}'.format(evals[0]))\n",
        "print('Accuracy: {}'.format(evals[1]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9561\n",
            "Loss: 0.14216303825378418\n",
            "Accuracy: 0.9561403393745422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YT374SZzxucv"
      },
      "source": [
        "The classes of the Wisconsin data set are not balanced, so we may want to check out the balanced accuracy as well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKJpwzLtxucw",
        "outputId": "70d55cd4-8d9b-413b-dcd8-5f3be8f1ca30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "labels = np.argmax(y_test, axis=1)\n",
        "idx0 = (labels == 0)\n",
        "idx1 = (labels == 1)\n",
        "\n",
        "acc0 = np.mean(np.equal(classes[idx0], labels[idx0]))\n",
        "acc1 = np.mean(np.equal(classes[idx1], labels[idx1]))\n",
        "bal_acc = (acc0 + acc1) / 2\n",
        "print('Balanced accuracy: {}'.format(bal_acc))\n",
        "print('\\tClass 0: {}'.format(acc0))\n",
        "print('\\tClass 1: {}'.format(acc1))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Balanced accuracy: 0.9489864864864865\n",
            "\tClass 0: 0.925\n",
            "\tClass 1: 0.972972972972973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymEb9XTTxucy"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "Before we trained the neural network, we preprocessed the data set by z-normalizing it. What happens if you remove this normalization step? Can you explain the observed behavior? Why do we go through the trouble of separately normalizing the training and test data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx0c9vtMxucy"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Keras supports a number of different loss functions (see [the documentation](https://keras.io/losses/)). Try modifying the loss function we used in the example to something else and retrain the network. Why would you choose a certain loss over another? In particular, why is the 0/1 loss not even on this list?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrh94gCNxucz"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Keras has a number of activation functions built in besides the RELU (see [the documentation](https://keras.io/activations/)). Change the activation functions from RELU to something else and retrain the network. Can you explain the effects of this choice? **Hint:** try visualizing the activation functions with a plot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1QCmty_xucz"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "Compare the performance of the neural network to more \"classical\" machine learning algorithms such as random forests or support vector machines. Can you get the network to outperform them all?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLRTUhmgxuc0"
      },
      "source": [
        "## Exercise 5\n",
        "\n",
        "Keras provides a [number of data sets](https://keras.io/datasets/) for you to experiment with. Try training a neural network on the Boston housing price regression data set. Note that this is a *regression problem*, not a *classification problem* as we have considered up until this point. You'll have to change your approach slightly to tackle this problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMLTsh4gxuc0"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}